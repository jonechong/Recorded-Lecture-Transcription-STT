{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with mainly 4 kind features: MFCC, Zero Crossing Rate, Spectral Centroid and Spectrograms. \n",
    "\n",
    "We start off with same data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "# Define the path to the folder containing your TSV files\n",
    "folder_path = r'C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\Voice dataset\\cv-corpus-4'\n",
    "\n",
    "# List of the TSV files you want to combine\n",
    "tsv_files = ['dev.tsv', 'invalidated.tsv', 'other.tsv', 'test.tsv', 'train.tsv', 'validated.tsv']\n",
    "\n",
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file and append its contents to the combined DataFrame\n",
    "for file_name in tsv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    temp_df = pd.read_csv(file_path, sep='\\t')\n",
    "    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_df.drop(['client_id', 'up_votes', 'down_votes', 'age', 'gender', 'accent'], axis=1, inplace=True)\n",
    "\n",
    "# Initialize the inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Check if the input is not a string (e.g., NaN or None)\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # or some placeholder text, e.g., \"missing_sentence\"\n",
    "    \n",
    "    # Convert numbers to words\n",
    "    text = re.sub(r'\\b\\d+\\b', lambda x: p.number_to_words(x.group()), text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep only spaces, lowercase letters, and numbers in word form\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalization including the check for non-string types\n",
    "combined_df['sentence'] = combined_df['sentence'].apply(normalize_text)\n",
    "\n",
    "# Proceed with saving the normalized labels as before\n",
    "combined_csv_path = os.path.join(folder_path, 'labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df[combined_df[\"path\"] == 'common_voice_en_10.mp3']\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = r'C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\Voice dataset\\cv-corpus-4'\n",
    "path_substring = \"common_voice_en_10.mp3\"\n",
    "clips_folder = os.path.join(base_folder, 'clips')\n",
    "\n",
    "for filename in os.listdir(clips_folder):\n",
    "    if path_substring in filename:\n",
    "        file_path = os.path.join(clips_folder, filename)\n",
    "        print(f\"Found and playing file: {file_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the MFCCs and store in h5py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the code below, if you are processing the entire corpus file, I do not recommend using jupyter notebook. Create a .py file and run that instead. The python file (feature_extraction.py) also considers feature extraction with melspectrograms, zero crossing rate and spectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def process_and_store_audio_files_with_labels(folder_path, hdf5_path, labels_dict, sampling_rate=16000, hop_length=512, n_mfcc=13):\n",
    "    with h5py.File(hdf5_path, 'w') as hdf5_file:\n",
    "        processed_files = 0\n",
    "        skipped_files = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            print(f\"Processing {processed_files + skipped_files}-th file: {filename}\")\n",
    "            if filename.endswith('.mp3') and filename in labels_dict:\n",
    "                if not isinstance(labels_dict[filename], str):\n",
    "                    continue\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    signal, _ = librosa.load(file_path, sr=sampling_rate)\n",
    "                    mfccs = librosa.feature.mfcc(y=signal, sr=sampling_rate, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {filename} due to error: {e}\")\n",
    "                    skipped_files += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create a group for each audio file\n",
    "                grp = hdf5_file.create_group(filename)\n",
    "                # Store MFCCs in the group\n",
    "                grp.create_dataset(\"mfccs\", data=mfccs)\n",
    "                # Store the label in the group\n",
    "                label_data = labels_dict[filename].encode('utf-8')\n",
    "                grp.create_dataset(\"label\", data=label_data)\n",
    "                \n",
    "                print(f\"Processed and stored MFCCs and label for {filename}\")\n",
    "                processed_files += 1\n",
    "        print(f\"Processed {processed_files} files\")\n",
    "        print(f\"Skipped {skipped_files} files\")\n",
    "\n",
    "# Params\n",
    "sampling_rate = 16000\n",
    "hop_length = 512\n",
    "n_mfcc = 13\n",
    "\n",
    "# Load the labels first\n",
    "\n",
    "label_file_path = r'C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\Voice dataset\\cv-corpus-4\\labels.csv'\n",
    "labels_df = pd.read_csv(label_file_path)\n",
    "labels_dict = pd.Series(labels_df.sentence.values, index=labels_df.path).to_dict()\n",
    "\n",
    "\n",
    "# Specify the path to save the HDF5 file\n",
    "hdf5_path = 'mfccs_dataset.h5'\n",
    "# Path to the folder containing your MP3 files\n",
    "folder_path = r'C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\Voice dataset\\cv-corpus-4\\clips'\n",
    "\n",
    "# Process the audio files and store their MFCCs\n",
    "process_and_store_audio_files_with_labels(folder_path, hdf5_path, labels_dict, sampling_rate, hop_length, n_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DEBUGGING BLOCK) Inspect the h5 file content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_contents_in_hdf5(hdf5_path):\n",
    "    with h5py.File(hdf5_path, 'r') as hdf5_file:\n",
    "        print(f\"Contents in {hdf5_path}:\")\n",
    "        # Iterate through groups in the HDF5 file\n",
    "        for group_name in hdf5_file:\n",
    "            print(f\"Group name: {group_name}\")\n",
    "            group = hdf5_file[group_name]\n",
    "            # Iterate through datasets in each group\n",
    "            for dataset_name in group:\n",
    "                dataset = group[dataset_name]\n",
    "                print(f\"  Dataset name: {dataset_name}\")\n",
    "                print(f\"  Shape: {dataset.shape}\")\n",
    "                print(f\"  Datatype: {dataset.dtype}\")\n",
    "            print(\"---\" * 10)\n",
    "\n",
    "# Specify the path to your HDF5 file\n",
    "hdf5_path = 'mfccs_dataset.h5'\n",
    "\n",
    "list_contents_in_hdf5(hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug functions \n",
    "\n",
    "# If you want to get labels for a specific audio\n",
    "def get_label_for_audio(hdf5_path, audio_filename):\n",
    "    with h5py.File(hdf5_path, 'r') as hdf5_file:\n",
    "        # Access the group corresponding to the audio file\n",
    "        audio_group = hdf5_file[audio_filename]\n",
    "        # Access the 'label' dataset within this group\n",
    "        label_data = audio_group['label'][()]\n",
    "        # Decode the binary string to get the label text\n",
    "        label_text = label_data.decode('utf-8')\n",
    "        return label_text\n",
    "\n",
    "# Specify the path to your HDF5 file\n",
    "hdf5_path = 'mfccs_dataset.h5'\n",
    "# Specify the audio filename group you're interested in\n",
    "audio_filename = 'common_voice_en_1.mp3'\n",
    "\n",
    "# Retrieve and print the label for the specified audio file\n",
    "label_text = get_label_for_audio(hdf5_path, audio_filename)\n",
    "print(f\"Label for {audio_filename}: {label_text}\")\n",
    "\n",
    "\n",
    "# If you want to get the length of the hdf5 file\n",
    "def get_length_hdf5(hdf5_path):\n",
    "    with h5py.File(hdf5_path, 'r') as hdf5_file:\n",
    "        # Count the number of top-level groups in the HDF5 file\n",
    "        number_of_groups = len(hdf5_file.keys())\n",
    "        return number_of_groups\n",
    "\n",
    "# Specify the path to your HDF5 file\n",
    "hdf5_path = 'mfccs_dataset.h5'\n",
    "\n",
    "# Get the count of groups (audio files) in the HDF5 file\n",
    "group_count = get_length_hdf5(hdf5_path)\n",
    "print(f\"The HDF5 file contains {group_count} groups (audio files).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUTD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
