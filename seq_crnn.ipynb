{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to train the model for the STT task with a sequential CRNN model. We will first feed the processed audio features to a convolutional layer. The output features map from the convolutional layer will then be fed to the recurrent layer, before finally leading to the output. In contrast, a parallel CRNN will have both convolutional layer and recurrent layer run in parallel, before feature fusion into an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import librosa\n",
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Building & Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    \"\"\"\n",
    "    Builds a fixed vocabulary of lowercase English letters, space, and a special\n",
    "    '<blank>' token for CTC.\n",
    "    Returns:\n",
    "        dict: A dictionary mapping characters to their integer encodings.\n",
    "    \"\"\"\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "    vocab = {\"<blank>\": 0}  # CTC blank token\n",
    "    for idx, char in enumerate(alphabet, start=1):  # Starting from 1 to reserve 0 for blank\n",
    "        vocab[char] = idx\n",
    "    return vocab\n",
    "\n",
    "def save_vocab(vocab, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "\n",
    "def load_vocab(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label, vocab):\n",
    "    return [vocab[char] for char in label]\n",
    "\n",
    "def decode_label(encoded_label, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return ''.join(inv_vocab[id] for id in encoded_label if id not in (0, 1))  # Skip pad and blank tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocab (use only if you don't already have the vocab built!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<blank>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ' ': 27}\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab()\n",
    "print(vocab)\n",
    "save_vocab(vocab, 'vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab('vocab.json')\n",
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, hdf5_path, vocab, max_length_frames=247):\n",
    "        super(SpeechDataset, self).__init__()\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.vocab = vocab\n",
    "        # Maximum sequence length for padding, 247 = 8s @ 16000 Hz, 512 hop length for MFCC\n",
    "        self.max_length_frames = max_length_frames  \n",
    "        \n",
    "        self.keys = []  # Initialize an empty list for valid keys\n",
    "        with h5py.File(hdf5_path, 'r') as file:\n",
    "            for key in file.keys():\n",
    "                # Check if 'label' exists for this key\n",
    "                if 'label' in file[key]:\n",
    "                    self.keys.append(key)\n",
    "                else:\n",
    "                    print(f\"Skipping {key} due to missing label.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.hdf5_path, 'r') as f:\n",
    "            key = self.keys[idx]\n",
    "            # Or use whatever features you need\n",
    "            # melspectrogram = np.array(f[key]['melspectrogram']).astype(np.float32)\n",
    "            mfccs = np.array(f[key]['mfccs']).astype(np.float32)\n",
    "            \n",
    "            # Calculate the number of padding frames needed\n",
    "            # padding_length = self.max_length_frames - melspectrogram.shape[1]\n",
    "            padding_length = self.max_length_frames - mfccs.shape[1]\n",
    "            if padding_length > 0:\n",
    "                # Pad the sequence to max_length_frames if it's shorter\n",
    "                # melspectrogram = np.pad(melspectrogram, ((0,0), (0, padding_length)), mode='constant', constant_values=0)\n",
    "                mfccs = np.pad(mfccs, ((0,0), (0, padding_length)), mode='constant', constant_values=0)\n",
    "            elif padding_length < 0:\n",
    "                # Truncate the sequence to max_length_frames if it's longer\n",
    "                # melspectrogram = melspectrogram[:, :self.max_length_frames]\n",
    "                mfccs = mfccs[:, :self.max_length_frames]\n",
    "\n",
    "            # melspectrogram = np.expand_dims(melspectrogram, 0)  # Shape: [1, Freq, Time]\n",
    "            mfccs = np.expand_dims(mfccs, 0)  # Shape: [1, Freq, Time]\n",
    "\n",
    "            label_str = f[key]['label'][()].decode('utf-8')\n",
    "            label = encode_label(label_str, self.vocab)\n",
    "            input_length = self.max_length_frames\n",
    "            label_length = len(label)\n",
    "\n",
    "        # return torch.tensor(melspectrogram), torch.tensor(label, dtype=torch.int), input_length, label_length\n",
    "        return torch.tensor(mfccs), torch.tensor(label, dtype=torch.int64), self.max_length_frames, torch.tensor(label_length, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_mfcc_features, hidden_size, num_layers=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.fc_out_size = VOCAB_SIZE  # Number of output classes, including the blank for CTC\n",
    "\n",
    "        # Convolutional layers with Batch Normalization and Dropout\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),  # BatchNorm after convolution\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),  # Dropout after pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # BatchNorm after convolution\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),  # Dropout after pooling\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the RNN's input. Assuming the input MFCCs have shape [Batch, 1, Time, Features]\n",
    "        # and after convolutions and pooling, the feature (height) dimension is reduced by a factor of 4,\n",
    "        # and the time (width) dimension is also reduced. The factor reduction in the time dimension depends on\n",
    "        # the length of your input sequences and the exact architecture of your convolutional layers.\n",
    "        self.rnn_input_size = 64 * (num_mfcc_features // 4)  # Adjust based on your pooling and convolution operations\n",
    "\n",
    "        # Recurrent layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, self.fc_out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = self.conv(x)  \n",
    "        # Prepare the output of the CNN for the RNN\n",
    "        batch, channels, height, width = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()  # Change to [Batch, Width, Channels, Height]\n",
    "        x = x.view(batch, width, -1)  # Flatten the feature maps\n",
    "        \n",
    "        # Apply RNN\n",
    "        output, _ = self.rnn(x)\n",
    "        \n",
    "        # Apply fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(output, labels, blank_label=0):\n",
    "    \"\"\"\n",
    "    Decodes the output of a CTC network and returns the string representation.\n",
    "    \n",
    "    Args:\n",
    "        output (torch.Tensor): The raw output from the CRNN model. Shape: [T, N, C] where\n",
    "            T is the timestep, N is the batch size, and C is the number of classes (including the blank).\n",
    "        labels (List[str]): The encoded labels as a list of strings.\n",
    "        label_lengths (torch.Tensor): The length of each label in the batch.\n",
    "        blank_label (int): The index of the blank label used in CTC. Defaults to 0.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: The decoded strings.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:  # Not a blank\n",
    "                if j != 0 and index == args[j - 1]:\n",
    "                    continue  # Repeated character\n",
    "                decode.append(index.item())\n",
    "        decodes.append(''.join([labels[k] for k in decode]))\n",
    "    return decodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, sampling_rate=16000, n_mfcc=13, max_length_frames=247):\n",
    "    # Load the audio file\n",
    "    signal, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # Extract MFCC features from the audio signal\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Pad or truncate the MFCC sequences to the fixed length\n",
    "    padding_length = max_length_frames - mfccs.shape[1]\n",
    "    if padding_length > 0:\n",
    "        # Pad the sequence if shorter\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding_length)), mode='constant', constant_values=0)\n",
    "    elif padding_length < 0:\n",
    "        # Truncate the sequence if longer\n",
    "        mfccs = mfccs[:, :max_length_frames]\n",
    "    \n",
    "    # Add a channel dimension and return\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Shape: [1, n_mfcc, max_length_frames]\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver/Loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    \"\"\"\n",
    "    Saves the model and training parameters at the specified checkpoint.\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    \"\"\"\n",
    "    Loads the model and training parameters from a specified checkpoint.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Collate Function\n",
    "This is necessary because the default data collation function (default_collate) attempts to stack all tensors in a batch along a new dimension, but this requires all tensors to have the same shape. But we are using CTC, so our tensors will be a little different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    mfccs, labels, input_lengths, label_lengths = zip(*batch)\n",
    "    \n",
    "    # Pad the mfcc sequences (this might be redundant with your existing padding, depending on your implementation)\n",
    "    mfccs_padded = pad_sequence(mfccs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad the label sequences to the maximum label length in the batch\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)  # Assumes 0 is an appropriate pad value\n",
    "    \n",
    "    input_lengths = torch.tensor(input_lengths, dtype=torch.long)\n",
    "    label_lengths = torch.tensor(label_lengths, dtype=torch.long)\n",
    "    \n",
    "    return mfccs_padded, labels_padded, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(target, prediction):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance between the\n",
    "    two given strings normalized by the length of the true string.\n",
    "    \"\"\"\n",
    "    char_error_rate = lev.distance(target, prediction) / max(len(target), 1)\n",
    "    return char_error_rate\n",
    "\n",
    "def wer(target, prediction):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    word sequences of the two given strings normalized by the number of words\n",
    "    in the true string.\n",
    "    \"\"\"\n",
    "    target_words = target.split()\n",
    "    prediction_words = prediction.split()\n",
    "    word_error_rate = lev.distance(' '.join(target_words), ' '.join(prediction_words)) / max(len(target_words), 1)\n",
    "    return word_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, device, train_loader, val_loader, optimizer, epochs, start_epoch=0):\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    # Define inv_vocab here\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        audio_no = 0\n",
    "        for mels, labels, input_lengths, label_lengths in train_loader:\n",
    "            mels = mels.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mels)\n",
    "            outputs = F.log_softmax(outputs, dim=2)\n",
    "            processed_seq_length = 61\n",
    "            input_lengths = torch.full(size=(mels.size(0),), fill_value=processed_seq_length, dtype=torch.long).to(device)\n",
    "            loss = criterion(outputs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            audio_no += 1\n",
    "            print(f'Epoch {epoch}, Audio Batch No: {audio_no} processed!')\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch}, Training Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_cer = 0.0\n",
    "        total_wer = 0.0\n",
    "        total_samples = 0\n",
    "        audio_no = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for mels, labels, input_lengths, label_lengths in val_loader:\n",
    "                mels = mels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(mels)\n",
    "                outputs = F.log_softmax(outputs, dim=2)\n",
    "                processed_seq_length = 61\n",
    "                input_lengths = torch.full(size=(mels.size(0),), fill_value=processed_seq_length, dtype=torch.long).to(device)\n",
    "                loss = criterion(outputs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Decoding and computing CER and WER\n",
    "                decoded_preds = greedy_decoder(outputs, inv_vocab, blank_label=vocab[\"<blank>\"])\n",
    "                for i, label_tensor in enumerate(labels):\n",
    "                    target = ''.join([inv_vocab.get(id.item(), '') for id in label_tensor if id.item() not in (0, 1)])\n",
    "                    prediction = decoded_preds[i]\n",
    "                    total_cer += cer(target, prediction)\n",
    "                    total_wer += wer(target, prediction)\n",
    "                total_samples += len(labels)\n",
    "                audio_no += 1\n",
    "                print(f'Epoch {epoch}, Audio Batch No: {audio_no} evaluated!')\n",
    "\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        avg_cer = total_cer / total_samples\n",
    "        avg_wer = total_wer / total_samples\n",
    "        print(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f}, CER: {avg_cer:.4f}, WER: {avg_wer:.4f}')\n",
    "        \n",
    "        # Save model at each epoch\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, filename=f\"./checkpoints/checkpoint_epoch_{epoch}.pth.tar\")\n",
    "\n",
    "        # Write metrics to a file\n",
    "        with open(\"training_metrics.txt\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, CER: {avg_cer:.4f}, WER: {avg_wer:.4f}\\n\")\n",
    "            # file.write(f\"Epoch {epoch}, Validation Loss: {avg_val_loss:.4f}, CER: {avg_cer:.4f}, WER: {avg_wer:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validate Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "def split_dataset(dataset, train_size=0.8):\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(dataset)), train_size=train_size, random_state=42)\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    return train_subset, val_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths declaration\n",
    "hdf5_path = r\"C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\STT\\Recorded-Lecture-Transcription-STT\\reduced_mfcc_dataset.h5\"\n",
    "\n",
    "# Model params declaration\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "num_mfcc_features = 13\n",
    "hidden_size = 256\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset and DataLoader instantiation\n",
    "dataset = SpeechDataset(hdf5_path, vocab)\n",
    "# Split your dataset\n",
    "train_subset, val_subset = split_dataset(dataset)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load the checkpoint file. Use only if you are indeed loading from a checkpoint\n",
    "# checkpoint = torch.load(r\"./checkpoints/checkpoint_epoch_14.pth.tar\") \n",
    "\n",
    "# Model initialisation\n",
    "model = CRNN(num_mfcc_features=num_mfcc_features, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Training execution\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load the model and optimizer state from the checkpoint. Again, use only if you are indeed loading from a checkpoint\n",
    "start_epoch = 0\n",
    "# load_checkpoint(checkpoint, model, optimizer)\n",
    "\n",
    "# Training and validation execution\n",
    "train_and_validate(model, device, train_loader, val_loader, optimizer, epochs, start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer & Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "audio_path = r\"C:\\Users\\jonec\\Documents\\SUTD\\T6\\AI\\Voice dataset\\cv-corpus-4\\clips\\common_voice_en_12.mp3\"\n",
    "mfccs = preprocess_audio(audio_path)\n",
    "mfccs_tensor = torch.tensor(mfccs).float()\n",
    "mfccs_tensor = mfccs_tensor.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(mfccs_tensor)\n",
    "\n",
    "# Assuming you have a list or dict `vocab` mapping indices to characters\n",
    "inv_vocab = {i: char for char, i in vocab.items()}\n",
    "decoded_output = greedy_decoder(output, inv_vocab, blank_label=vocab[\"<blank>\"])\n",
    "\n",
    "print(\"Transcription:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUTD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
